{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CYK\n",
        "\n",
        "In this assignment, we'll implement CKY.  See the lecture notes and async material for a detailed discussion of the algorithm.\n",
        "\n",
        "We'll write code in three parts:\n",
        "1.  Initial preprocessing of the treebank\n",
        "2.  Calculation of production rule probabilities\n",
        "3.  CYK itself\n",
        "\n",
        "We provide the code for 1 and much of the framework surrounding 2 and 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import some useful libraries...\n",
        "import collections\n",
        "import copy\n",
        "import math\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Preprocessing\n",
        "\n",
        "This first step of preprocessing takes the treebank, strips out the cross references (NPs are wrapped by special nodes that assign index numbers to them so that coreferences can be indicated).  Unfortunately, this also injects a NP-SBJ-# node between nodes you'd expect to produce one another.  Since the # changes throughout the corpus, our counts of the production rules all end up being 1 - and useless.\n",
        "\n",
        "See NP-SBJ-1 in the tree below.  Note there is also a NP-SBJ leading to a NONE/1 subtree as a crossreference later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nltk.corpus.treebank.parsed_sents()[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code below we skip over nodes whose label start with NP-, connecting any children nodes to the NP-'s parent.  We also snip out any subtrees rooted by NONE.  The tree above is printed again after this next cell to illustrate the effect of this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Preprocess the treebank.\n",
        "def get_real_child(node):\n",
        "    if type(node) == types.UnicodeType:\n",
        "        return [node]\n",
        "    if 'NONE' in node.label():\n",
        "        return []\n",
        "   \n",
        "    real_children = []\n",
        "    if node.label().startswith('NP-'):\n",
        "        for child in node:\n",
        "            real_children.extend(get_real_child(child))\n",
        "    else:\n",
        "        real_children.append(node)\n",
        "    return [copy_strip_np_sbj(x) for x in real_children]\n",
        "\n",
        "def copy_strip_np_sbj(sentence):\n",
        "    children = []\n",
        "    for child in sentence:\n",
        "        children.extend(get_real_child(child))\n",
        "    return Tree(sentence.label(), children)\n",
        "\n",
        "pre_chomsky = []\n",
        "sentences = []\n",
        "for sentence in nltk.corpus.treebank.parsed_sents():\n",
        "    # Filter out NP-* nodes.\n",
        "    filtered_sentence = copy_strip_np_sbj(sentence)\n",
        "    pre_chomsky.append(filtered_sentence)\n",
        "    \n",
        "    # Convert sentence to Chomsky normal form.\n",
        "    transformed_sentence = copy.deepcopy(filtered_sentence)\n",
        "    nltk.treetransforms.chomsky_normal_form(transformed_sentence, horzMarkov=2)\n",
        "    \n",
        "    # Add final sentence to list.\n",
        "    sentences.append(transformed_sentence)\n",
        "    \n",
        "pre_chomsky[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, CYK assumes that trees are constructed from a grammar that is in [Chomsky normal form](https://en.wikipedia.org/wiki/Chomsky_normal_form).\n",
        "\n",
        "This means that the grammar only consists of rules:\n",
        "- A -> BC\n",
        "- A -> a\n",
        "- S -> $\\epsilon$\n",
        "\n",
        "where A, B, C, S are non-terminals and a is a terminal.  $\\epsilon$ is the empty sentence.\n",
        "\n",
        "In order to accomplish this, we add new non-terminals to the language and build longer sequences of non-terminals through them.  Concretely,\n",
        "- A -> BCD\n",
        "\n",
        "becomes\n",
        "- A -> BE\n",
        "- E -> CD\n",
        "\n",
        "Adding all these non-terminals with opaque names starts getting confusing, so one notation that's popular is using \"A|C-D\" as the name of the new terminal instead of \"E\".\n",
        "\n",
        "This works pretty well until you have grammar rules like:\n",
        "- A -> BCDEFGHIJKL\n",
        "\n",
        "In which case you'd induce a new node: A|B-C-D-E-F-G-H-... and week 2's sparcity concerns should be coming to mind.  To keep our counts relatively large, there is a hyperparameter we can pick (analogous to the n of n-gram) called the horizontal markovization parameter.  It does just what you'd expect: it controls how the number of symbols after the pipe in the node name.  This allows evidence to collect across more examples that are similar in structure.\n",
        "\n",
        "Take a minute to play with the ```horzMarkov``` parameter in the block above to see how this works. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sentences[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Production rule probabilities\n",
        "\n",
        "In this next section, you'll compute about production rule probabilities.\n",
        "\n",
        "Remember that a production rule now looks like this:\n",
        "- A -> BC; or,\n",
        "- A -> a\n",
        "\n",
        "The left hand side (LHS) of these rules only ever consist of a single non-terminal.  The right hand side (RHS) consists of two non-terminals or one terminal.\n",
        "\n",
        "We'll do this in two stages:\n",
        "- Count LHS, and (LHS,RHS) each in their own dict\n",
        "- Calculate $P(RHS | LHS) = \\frac{count(LHS, RHS)}{count(LHS)}$\n",
        "\n",
        "Before we get started though, let's play a bit with the NLTK API.\n",
        "\n",
        "### TODO\n",
        "In the next cell, take sentence[0] and display it like we do above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO\n",
        "\n",
        "In the next cell, print the label of the root of the sentence and also all the labels of the child nodes (note, there should only be two children due to the normalization done above).\n",
        "\n",
        "Hint:  The \"sentence\" object is a [Tree](http://www.nltk.org/_modules/nltk/tree.html).  See the iteration and label methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO\n",
        "\n",
        "Output:\n",
        "- all the production rules found in this sentence.\n",
        "- the left hand side of the first production.\n",
        "- the right hand side of the second production.\n",
        "\n",
        "Hint:  There is a one-line solution to all.  See the Tree API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO\n",
        "\n",
        "With that API fun out of the way, loop over all the sentences and fill production_counts with a count of each LHS, RHS pair and lhs_counts with the number of times you've seen each non-terminal on the LHS.\n",
        "\n",
        "If everything works, you should see this in the cell below\n",
        "\n",
        "```[(, -> ',', 4885),\n",
        " (PP -> IN NP, 4045),\n",
        " (DT -> 'the', 4038),\n",
        " (. -> '.', 3828),\n",
        " (S|<VP-.> -> VP ., 3018),\n",
        " (IN -> 'of', 2319),\n",
        " (NP -> NP PP, 2188),\n",
        " (TO -> 'to', 2161),\n",
        " (NP -> DT NN, 2020),\n",
        " (DT -> 'a', 1874)]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "production_counts = collections.Counter()\n",
        "lhs_counts = collections.Counter()\n",
        "# YOUR CODE HERE\n",
        "# END YOUR CODE\n",
        "sorted([x for x in production_counts.iteritems()], key=lambda x: x[1], reverse=True)[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO\n",
        "Compute the probability of each potential RHS given the LHS.\n",
        "\n",
        "Hint: As usual, we run into numerical issues when multiplying probabilities.  You should take the usual approach here: use math.log(numerator) - math.log(denominator) and add log-probabilities together instead of multipling probabilities.\n",
        "\n",
        "The final result of this cell, scored_productions, should be a dict mapping from RHS -> [(LHS_1, log_probability_1), (LHS_2, log_probability_2), ...]\n",
        "\n",
        "Each LHS is the left hand side of a production rule that can create the RHS along with the probability of it doing so.  We key this table by RHS instead of LHS as CYK builds its chart from the bottom up (and thus we'll be looking up RHS-s and trying to combine them into LHS-s).\n",
        "\n",
        "If everything went well, you should see:\n",
        "```\n",
        "food [(NN, -6.71280430578804)]\n",
        "a [(IN, -9.19593714166544), (DT, -1.4717815426061982), (LS, -2.5649493574615367), (JJ, -7.978310969867721)]\n",
        "I [(NNP, -8.45638105201948), (PRP, -2.720363461335567)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scored_productions = collections.defaultdict(list)\n",
        "# YOUR CODE HERE\n",
        "# END YOUR CODE\n",
        "for w in ['food', 'a', 'I']:\n",
        "    print w, scored_productions[(w,)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You don't need to do anything with this next cell except to run it.\n",
        "\n",
        "It's not particularly useful, but if you need to keep track of what each variable contains, this provides a useful reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print 'Productions:'\n",
        "for production, count in [x for x in production_counts.iteritems()][0:5]:\n",
        "    print production, count, type(production)\n",
        "\n",
        "print '\\n\\nLHS counts:'\n",
        "for lhs, count in sorted(lhs_counts.iteritems())[:5]:\n",
        "    print lhs, count\n",
        "    \n",
        "print '\\n\\nLog Probabilities:'\n",
        "print '\\n'.join([str(x) for x in scored_productions.iteritems()][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implement CYK!\n",
        "\n",
        "After that bit of preamble, you only have one more cell to go!  It's a big one though, so do take your time and get things right.\n",
        "\n",
        "We've set up the chart for you.  Concretely \"chart\" is a dict that you can index into first by cell position and then by non-terminal like this:\n",
        "\n",
        "```chart[(0, 1)][NN]```\n",
        "\n",
        "The value is a tuple (log_probability, back_trace_tree).\n",
        "\n",
        "Construct the back_trace_tree by calling Tree(non_terminal, [its, children]).\n",
        "\n",
        "### TODO: Implement CYK.\n",
        "\n",
        "HINT: it isn't strictly necessary, but it can be convenient to split the task into first mapping words to options of pre-terminals (i.e. bottom row of the chart) and then build the rest of the chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def CYK(words):\n",
        "    '''Accept a list of words and return a tuple (score, Tree) where Tree contains the parse with score score.'''\n",
        "    cell_creator = lambda: collections.defaultdict(lambda:(-float('inf'), Tree('unknown', [])))\n",
        "    chart = collections.defaultdict(cell_creator)\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "        \n",
        "    # END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score_tree = CYK('I eat red hot food with a knife'.split())\n",
        "assert round(score_tree[0], 2) == -64.89\n",
        "score_tree[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO\n",
        "\n",
        "Try a few more sentences.  Do you notice any patterns with your results?  Any common types of errors?  Are these an artifact of CYK, or of how you did the markovization/counting?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
