{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update:** We've improved the model schematic, and added clarifications to the documentation in both this notebook and in `rnnlm.py`. Fetch the latest version from GitHub, or look at the diffs to see what's new.\n",
    "\n",
    "**Update:** We've provided the solution implementation of `MakeFancyRNNCell`; hopefully this will make things easier! For an illustration of how a multi-layer RNN cell works, see [this diagram](RNNLM - multicell.png). The \"cell\" from `MakeFancyRNNCell` with `num_layers = 2` is the unit inside the dashed green box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Part 2: RNN Language Model\n",
    "**(45 points total)**\n",
    "\n",
    "In this part of the assignment, you'll implement a recurrent neural network language model. This class of models represents the cutting edge in language modeling, and what you implement will be include many of the same features.\n",
    "\n",
    "As a reference, you may want to review the following papers:\n",
    "\n",
    "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
    "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
    "\n",
    "You'll be writing a fair amount of TensorFlow code, so you may want to review the [Week 1 TensorFlow tutorial](../week1/TensorFlow%20Tutorial.ipynb) or the [Week 4 notebook](../week4/Neural%20Probabilistic%20Language%20Model.ipynb) for review. For documentation on specific functions, consult the [TensorFlow API reference](https://www.tensorflow.org/versions/r0.10/api_docs/python/index.html).\n",
    "\n",
    "Be sure you're using a recent installation of **TensorFlow version 0.9.0 or higher**.\n",
    "\n",
    "#### Note on Indentation\n",
    "\n",
    "This notebook (as well as rnnlm.py) uses 2-space indentation, instead of the 4 spaces that is Python's default. Why? It makes lines shorter, which is handy if you have a lot of nested scopes. Some people will yell at you for doing this, but the instructors don't care either way.\n",
    "\n",
    "You can follow [these instructions](http://jupyter-notebook.readthedocs.io/en/latest/frontend_config.html#example-changing-the-notebook-s-default-indentation) to configure Jupyter to be cool about this. In Chrome, you can open a JavaScript console by going to Menu -> More Tools -> Developer Tools and clicking the \"Console\" tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Overview\n",
    "\n",
    "- **(a)** RNNLM Inputs and Parameters (written questions)\n",
    "- **(b)** Implementing the RNNLM\n",
    "- **(c)** Training your RNNLM\n",
    "- **(d)** Sampling Sentences\n",
    "- **(e)** Linguistic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnlm; reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLM Model\n",
    "\n",
    "![RNNLM](RNNLM - layers.png)\n",
    "\n",
    "Here's the basic spec for our model. We'll use the following notation:\n",
    "\n",
    "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
    "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 4.8 of the async.\n",
    "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
    "- $y^{(i)}$ for the $i^{th}$ target word, which for a language model is always equal to $w^{(i+1)}$\n",
    "\n",
    "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
    "\n",
    "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $\n",
    "- **Recurrent layer:** $ (h^{(i)}, o^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
    "- **Output layer:** $\\hat{P}(y^{(i)}) = \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
    " \n",
    "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM, or even a stacked multi-layer cell.\n",
    "\n",
    "We'll use these as shorthand for important dimensions:\n",
    "- `V` : vocabulary size\n",
    "- `H` : hidden state size = embedding size\n",
    "\n",
    "It may be convenient to deal with the logits of the output layer, which are the un-normalized inputs to the softmax:\n",
    "$$ \\text{logits}^{(i)} = o^{(i)}W_{out} + b_{out} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters (9 points)\n",
    "\n",
    "**(written - no code)** Write your answers in the markdown cell below.\n",
    "\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).\n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see Section 4.8). Write the functional form in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors.)\n",
    "\n",
    "3. How many floating point operations are required to compute $\\hat{P}(w^{(i+1)})$ for a given target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for all target words?\n",
    "\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s) \\ge 0.5$ at each split $s$ in the tree.*)\n",
    "\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
    "\n",
    "1. *Your text here!*\n",
    "2. *Your text here!*\n",
    "3. *Your text here!*\n",
    "4. *Your text here!*\n",
    "5. *Your text here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and Truncated BPTT\n",
    "\n",
    "In theory, we model our RNN as operating on a single sequence of arbitrary length. During training, we'd run the RNN over that entire sequence, then backpropagate the errors from all the training labels. \n",
    "\n",
    "In practice, however, it's more common to operate on batches, and to perform *truncated backpropagation* on a fixed-length slice. This means our inputs $w$ and targets $y$ will be 2D arrays of shape `[batch_size, max_time]`:\n",
    "\n",
    "![RNNLM - batching](RNNLM - batching.png)\n",
    "\n",
    "Batching for an RNN means we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect the batch size as the first dimension.\n",
    "\n",
    "*Truncated backpropagation* through time means that we'll chop our sequences into blocks of fixed length - say, 20 - and run the RNN for only a fixed number `max_time` steps before we backpropagate errors. We'll still keep the final hidden state so that we can keep computing over the sequence, but we won't backpropagate across this boundary. For example, if our input was the sequence `1 2 3 4 5 6 7 8 9 10 11`, and `max_time=5`, we'd do:\n",
    "\n",
    "- Initialize `h0`\n",
    "- Run on block `[1 2 3 4 5]`\n",
    "- Backprop errors from targets `[2 3 4 5 6]`\n",
    "- Set `h0 = h_final`\n",
    "- Run on block `[6 7 8 9 10]`\n",
    "- Backprop errors from targets `[7 8 9 10 11]`\n",
    "\n",
    "And so on for longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM (21 points)\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you.\n",
    "\n",
    "Particularly, you'll need to implement three functions:\n",
    "- `BuildCoreGraph()` : the main RNN itself\n",
    "- `BuildTrainGraph()` : the training operations, including `train_loss_`, and `train_step_`\n",
    "- `BuildSamplerGraph()` : operations to generate output samples (`pred_samples_`)\n",
    "\n",
    "See `rnnlm.py` for more documentation.\n",
    "\n",
    "### Notes and Tips\n",
    "**`BuildCoreGraph`**\n",
    "We recommend implementing the `MakeFancyRNNCell` function as a wrapper to construct LSTM cells with (optional) dropout and multi-layer cells.\n",
    "\n",
    "You should use **`tf.nn.dynamic_rnn`** to build your recurrent layer. It takes care of making the recurrent connections and ensuring that the computation is done in the right (temporal) order, and gives you a nice wrapper that can take inputs of shape `[batch_size, max_time, ...]`.\n",
    "\n",
    "You'll need to provide initializations for your variables in the embedding layer and the output layer; we recommend random uniform or Xavier initialization (as in Part 0). The `tf.nn.rnn_cell` functions will automatically handle initialization of the internal cell variables (i.e. the LSTM matricies).\n",
    "\n",
    "**`BuildTrainGraph`**  \n",
    "You can use the softmax loss from `BuildCoreGraph` here, but we strongly recommend implementing an approximate (e.g. sampled) loss function for `train_loss_`. This will greatly speed up training.\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend  `tf.train.GradientDescentOptimizer` with gradient clipping (`tf.clip_by_global_norm`) or `tf.train.AdagradOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the following API functions useful:\n",
    "- [tf.nn.rnn_cell](https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell.html#RNNCell) (particularly `cell.zero_state`)\n",
    "- [tf.nn.dynamic_rnn](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#dynamic_rnn)\n",
    "- [tf.nn.sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits)\n",
    "- [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss)\n",
    "- [tf.multinomial](https://www.tensorflow.org/versions/r0.11/api_docs/python/constant_op.html#multinomial)\n",
    "\n",
    "Additionally, you can expect to make heavy use of [tf.shape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#shape) and [tf.reshape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#reshape). Note especially that you can use `-1` as a dimension in `tf.reshape` to automatically infer the size. For example:\n",
    "```\n",
    "x = tf.zeros([5,10], dtype=tf.float32)\n",
    "x.reshape([-1,])  # shape [50,]\n",
    "x.reshape([1, -1])  # shape [1, 50]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89e670ac90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e671dad0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import rnnlm; reload(rnnlm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "  lm.BuildCoreGraph()\n",
    "  lm.BuildTrainGraph()\n",
    "  lm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment1/part2\n",
    "tensorboard --logdir tf_summaries --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/#graphs to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 4 notebook.\n",
    "\n",
    "Particularly, `utils.batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w_0   w_1   w_2  w_3\n",
       "0  <s>  Mary   had    a\n",
       "1  <s>   The  lamb  was"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      w_0   w_1   w_2\n",
       "0  little  lamb     .\n",
       "1   white    as  snow"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_0   y_1  y_2     y_3\n",
       "0  Mary   had    a  little\n",
       "1   The  lamb  was   white"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_0   y_1  y_2\n",
       "0  lamb     .  <s>\n",
       "1    as  snow    ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "print \"Input words:\"\n",
    "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "  utils.pretty_print_matrix(w, cols=[\"w_%d\" % d for d in range(w.shape[1])], dtype=object)\n",
    "\n",
    "print \"Target words:\"\n",
    "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "  utils.pretty_print_matrix(y, cols=[\"y_%d\" % d for d in range(w.shape[1])], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = lm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = lm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "      h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    \n",
    "    # my actual code\n",
    "    feed_dict = {lm.input_w_: w,\n",
    "                 lm.target_y_: y}\n",
    "    cost, _ = session.run([lm.train_loss_, lm.train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "\n",
    "    \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `./tf_saved/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in under 10 minutes on a 16-core Google Cloud Compute instance, and reach a training set perplexity below 200.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "git add tf_saved/rnnlm_trained tf_saved/rnnlm_trained.meta\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (924077 tokens)\n",
      "Test set: 11468 sentences (237115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=500, \n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = 'tf_saved/rnnlm_trained_two_layers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(lm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89e5157590>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e65f6b50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1000, 'num_layers': 1, 'V': 10000}\n",
      "[epoch 1] Starting epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-574c198c060a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Run a training epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#### END(YOUR CODE) ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-da700488d4e1>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batch_iterator, train, verbose, tick_s, keep_prob, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m                  lm.target_y_: y}\n\u001b[1;32m     29\u001b[0m     cost, _ = session.run([lm.train_loss_, lm.train_step_],\n\u001b[0;32m---> 30\u001b[0;31m                        feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lsuttle/anaconda3/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lsuttle/anaconda3/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lsuttle/anaconda3/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/lsuttle/anaconda3/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lsuttle/anaconda3/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    print(model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # Run a training epoch.\n",
    "    run_epoch(lm, session, bi)\n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, 'tf_saved/rnnlm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    lm : rnnlm.RNNLM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  \n",
    "\n",
    "  feed_dict = {lm.input_w_: input_w,\n",
    "                 lm.initial_h_: initial_h}\n",
    "  samples = session.run(lm.pred_samples_,\n",
    "                       feed_dict=feed_dict)\n",
    "  final_h = session.run(lm.initial_h_, {lm.input_w_: input_w})\n",
    "  print(final_h)\n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89df445310>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e641f210>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[-0.76451647  0.46112311  0.4186615  ...,  0.97229308 -0.3914285\n",
      "  -0.13763683]\n",
      " [-0.11117601 -0.65054297  0.39880252 ..., -0.40079951 -0.73166656\n",
      "  -0.11502719]\n",
      " [-0.15344691  0.34534913  0.48810709 ..., -0.23984841  0.27369732\n",
      "  -0.01298911]\n",
      " ..., \n",
      " [-0.88014275 -0.73058897  0.50995725 ...,  0.62222534 -0.0968725\n",
      "   0.64549226]\n",
      " [ 0.85779375  0.09328027  0.03330379 ...,  0.90717453  0.80064046\n",
      "  -0.75497067]\n",
      " [ 0.10800394 -0.95510483 -0.54842532 ...,  0.71864265  0.36267427\n",
      "   0.36715195]]\n",
      "[[-0.76451647  0.46112311  0.4186615  ...,  0.97229308 -0.3914285\n",
      "  -0.13763683]\n",
      " [-0.11117601 -0.65054297  0.39880252 ..., -0.40079951 -0.73166656\n",
      "  -0.11502719]\n",
      " [-0.15344691  0.34534913  0.48810709 ..., -0.23984841  0.27369732\n",
      "  -0.01298911]\n",
      " ..., \n",
      " [-0.88014275 -0.73058897  0.50995725 ...,  0.62222534 -0.0968725\n",
      "   0.64549226]\n",
      " [ 0.85779375  0.09328027  0.03330379 ...,  0.90717453  0.80064046\n",
      "  -0.75497067]\n",
      " [ 0.10800394 -0.95510483 -0.54842532 ...,  0.71864265  0.36267427\n",
      "   0.36715195]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-61cee063431b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# We'll take one step for each sequence on each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-27de7a178c3b>\u001b[0m in \u001b[0;36msample_step\u001b[0;34m(lm, session, input_w, initial_h)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m#### END(YOUR CODE) ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(random_seed)\n",
    "\n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "\n",
    "  # Load the trained model\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, trained_filename)\n",
    "\n",
    "  # Make initial state for a batch with batch_size = num_samples\n",
    "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "  print(w[:,-1:])\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  print(h)\n",
    "  # We'll take one step for each sequence on each iteration \n",
    "  for i in xrange(max_steps):\n",
    "    h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "    w = np.hstack((w,y))\n",
    "\n",
    "  # Print generated sentences\n",
    "  for row in w:\n",
    "    for i, word_id in enumerate(row):\n",
    "      print vocab.id_to_word[word_id],\n",
    "      if (i != 0) and (word_id == vocab.START_ID):\n",
    "        break\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "  \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  feed_dict = {lm.input_w_:w,\n",
    "               lm.target_y_:y,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "  # Return log(P(seq)) = -1*loss\n",
    "  return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "  \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "      lm = rnnlm.RNNLM(**model_params)\n",
    "      lm.BuildCoreGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, trained_filename)\n",
    "  \n",
    "    if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
    "      inputs = [inputs]\n",
    "\n",
    "    # Actually run scoring\n",
    "    results = []\n",
    "    for words in inputs:\n",
    "      score = score_seq(lm, session, words, vocab)\n",
    "      results.append((score, words))\n",
    "    \n",
    "    # Sort if requested\n",
    "    if sort: results = sorted(results, reverse=True)\n",
    "    \n",
    "    # Print results\n",
    "    for score, words in results:\n",
    "      print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89e4254050>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e6376f10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"once upon a time\" : -23.75\n",
      "\"the quick brown fox jumps over the lazy dog\" : -62.66\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89e6076c50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e5dd5d90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the boy and the girl are\" : -25.81\n",
      "\"the boy and the girl is\" : -24.40\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\",\n",
    "        \"the boy and the girl is\"]\n",
    "\n",
    "load_and_score([s.split() for s in sents])\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "*Answer to above question(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89dc7b6b90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e5d07750>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"peanuts are my favorite kind of nut\" : -45.66\n",
      "\"peanuts are my favorite kind of vegetable\" : -45.44\n",
      "\"when I'm hungry I really prefer to eat\" : -56.11\n",
      "\"when I'm hungry I really prefer to drink\" : -56.86\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "        \"peanuts are my favorite kind of vegetable\",\n",
    "        \"when I'm hungry I really prefer to eat\",\n",
    "        \"when I'm hungry I really prefer to drink\"]\n",
    "\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "*Answer to above question(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from week2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f89e544bb50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f89e544ca10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I have lots of green square plastic toys\" : -65.42\n",
      "\"I have lots of plastic square green toys\" : -65.56\n",
      "\"I have lots of plastic green square toys\" : -65.59\n",
      "\"I have lots of green plastic square toys\" : -65.90\n",
      "\"I have lots of square plastic green toys\" : -66.05\n",
      "\"I have lots of square green plastic toys\" : -66.18\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
